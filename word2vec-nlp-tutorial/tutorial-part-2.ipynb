{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Meets Bags of Popcorn\n",
    "\n",
    "# 튜토리얼 파트 2 Word Vectors\n",
    "\n",
    "* 딥러닝 기법인 Word2Vec을 통해 단어를 벡터화 해본다. \n",
    "* t-SNE를 통해 벡터화 한 데이터를 시각화 해본다.\n",
    "* 딥러닝과 지도학습의 랜덤포레스트를 사용하는 하이브리드 방식을 사용한다.\n",
    "\n",
    "## Word2Vec(Word Embedding to Vector)\n",
    "\n",
    "컴퓨터는 숫자만 인식할 수 있고 한글, 이미지는 바이너리 코드로 저장 된다.\n",
    "튜토리얼 파트1에서는  Bag of Word라는 개념을 사용해서 문자를 벡터화 하여 머신러닝 알고리즘이 이해할 수 있도록 벡터화 해주는 작업을 하였다.\n",
    "\n",
    "\n",
    "* one hot encoding(예 [0000001000]) 혹은 Bag of Word에서 vector size가 매우 크고 sparse 하므로 neural net 성능이 잘 나오지 않는다.\n",
    "* `주위 단어가 비슷하면 해당 단어의 의미는 유사하다` 라는 아이디어\n",
    "* 단어를 트레이닝 시킬 때 주위 단어를 label로 매치하여 최적화\n",
    "* 단어를 `의미를 내포한 dense vector`로 매칭 시키는 것\n",
    "\n",
    "* Word2Vec은 분산 된 텍스트 표현을 사용하여 개념 간 유사성을 본다. 예를 들어, 파리와 프랑스가 베를린과 독일이 (수도와 나라) 같은 방식으로 관련되어 있음을 이해한다.\n",
    "\n",
    "![word2vec](https://1.bp.blogspot.com/-Q7F8ulD6fC0/UgvnVCSGmXI/AAAAAAAAAbg/MCWLTYBufhs/s1600/image00.gif)\n",
    "이미지 출처 : https://opensource.googleblog.com/2013/08/learning-meaning-behind-words.html\n",
    "\n",
    "* 단어의 임베딩과정을 실시간으로 시각화 : [word embedding visual inspector](https://ronxin.github.io/wevi/)\n",
    "\n",
    "\n",
    "![CBOW와 Skip-Gram](https://i.imgur.com/yXY1LxV.png)\n",
    "출처 : https://arxiv.org/pdf/1301.3781.pdf\n",
    " Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n",
    "\n",
    "\n",
    "* CBOW와 Skip-Gram기법이 있다.\n",
    "\n",
    "    * CBOW(continuous bag-of-words)는 전체 텍스트로 하나의 단어를 예측하기 때문에 작은 데이터셋일 수록 유리하다.    \n",
    "    * 아래 예제에서 __ 에 들어갈 단어를 예측한다.\n",
    "<pre>\n",
    "1) __가 맛있다. \n",
    "2) __를 타는 것이 재미있다. \n",
    "3) 평소보다 두 __로 많이 먹어서 __가 아프다.\n",
    "</pre>\n",
    "\n",
    "    * Skip-Gram은 타겟 단어들로부터 원본 단어를 역으로 예측하는 것이다. CBOW와는 반대로 컨텍스트-타겟 쌍을 새로운 발견으로 처리하고 큰 규모의 데이터셋을 가질 때 유리하다.\n",
    "    * `배`라는 단어 주변에 올 수 있는 단어를 예측한다.\n",
    "    \n",
    "    <pre>\n",
    "    1) *배*가 맛있다. \n",
    "    2) *배*를 타는 것이 재미있다. \n",
    "    3) 평소보다 두 *배*로 많이 먹어서 *배*가 아프다.\n",
    "    </pre>\n",
    "\n",
    "\n",
    "\n",
    "## Word2Vec 참고자료\n",
    "\n",
    "* [word2vec 모델 · 텐서플로우 문서 한글 번역본](https://tensorflowkorea.gitbooks.io/tensorflow-kr/g3doc/tutorials/word2vec/)\n",
    "* [Word2Vec으로 문장 분류하기 · ratsgo's blog](https://ratsgo.github.io/natural%20language%20processing/2017/03/08/word2vec/)\n",
    "\n",
    "* [Efficient Estimation of Word Representations in\n",
    "Vector Space](https://arxiv.org/pdf/1301.3781v3.pdf)\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "* [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/syllabus.html)\n",
    "* [Word2Vec Tutorial - The Skip-Gram Model · Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "## Gensim\n",
    "\n",
    "* [gensim: models.word2vec – Deep learning with word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "* [gensim: Tutorials](https://radimrehurek.com/gensim/tutorial.html)\n",
    "* [한국어와 NLTK, Gensim의 만남 - PyCon Korea 2015](https://www.lucypark.kr/docs/2015-pyconkr/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력이 너무 길어지지 않게하기 위해 찍지 않도록 했으나 \n",
    "# 실제 학습 할 때는 아래 두 줄을 주석처리 하는 것을 권장한다.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "(25000, 2)\n",
      "(50000, 2)\n",
      "25000\n",
      "25000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('data/labeledTrainData.tsv', \n",
    "                    header=0, delimiter='\\t', quoting=3)\n",
    "test = pd.read_csv('data/testData.tsv', \n",
    "                   header=0, delimiter='\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv('data/unlabeledTrainData.tsv', \n",
    "                              header=0, delimiter='\\t', quoting=3)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(unlabeled_train.shape)\n",
    "\n",
    "print(train['review'].size)\n",
    "print(test['review'].size)\n",
    "print(unlabeled_train['review'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train에 있는 평점정보인 sentiment가 없다.\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KaggleWord2VecUtility import KaggleWord2VecUtility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with', 'all', 'this', 'stuff', 'go', 'down', 'at', 'the', 'moment', 'with']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KaggleWord2VecUtility.review_to_wordlist(train['review'][0])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in train[\"review\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(\n",
    "        review, remove_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(\n",
    "        review, remove_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 모델을 학습\n",
    "전처리를 거쳐 파싱된 문장의 목록으로 모델을 학습시킬 준비가 되었다.\n",
    "\n",
    "## Gensim\n",
    "* [gensim: models.word2vec – Deep learning with word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "### Word2Vec 모델의 파라메터\n",
    "\n",
    "* 아키텍처 : 아키텍처 옵션은 skip-gram (default) 또는  CBOW 모델이다. skip-gram (default)은 느리지 만 더 나은 결과를 낸다.\n",
    "\n",
    "* 학습 알고리즘 : Hierarchical softmax (default) 또는 negative 샘플링. 여기에서는 기본값이 잘 동작한다.\n",
    "\n",
    "* 빈번하게 등장하는 단어에 대한 다운 샘플링 : Google 문서는 .00001에서 .001 사이의 값을 권장한다. 여기에서는 0.001에 가까운 값이 최종 모델의 정확도를 높이는 것으로 보여진다.\n",
    "\n",
    "* 단어 벡터 차원 : 많은 feature를 사용한다고 항상 좋은 것은 아니지만 대체적으로 좀 더 나은 모델이 된다. 합리적인 값은 수십에서 수백 개가 될 수 있고 여기에서는 300으로 지정했다.\n",
    "\n",
    "* 컨텍스트 / 창 크기 : 학습 알고리즘이 고려해야하는 컨텍스트의 단어 수는 얼마나 될까? hierarchical softmax 를 위해 좀 더 큰 수가 좋지만 10 정도가 적당하다. \n",
    "\n",
    "* Worker threads : 실행할 병렬 프로세스의 수로 컴퓨터마다 다르지만 대부분의 시스템에서 4에서 6 사이의 값을 사용하다.\n",
    "\n",
    "* 최소 단어 수 : 어휘의 크기를 의미있는 단어로 제한하는 데 도움이 된다. 모든 문서에서이 여러 번 발생하지 않는 단어는 무시된다. 10에서 100 사이가 적당하며, 이 경진대회의 데이터는 각 영화가 30개씩의 리뷰가 있기 때문에 개별 영화 제목에 너무 많은 중요성이 붙는 것을 피하기 위해 최소 단어 수를 40으로 설정한다. 그 결과 전체 어휘 크기는 약 15,000 단어가 된다. 높은 값은 제한 된 실행시간에 도움이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라메터값 지정\n",
    "num_features = 300 # 문자 벡터 차원 수\n",
    "min_word_count = 40 # 최소 문자 수\n",
    "num_workers = 4 # 병렬 처리 스레드 수\n",
    "context = 10 # 문자열 창 크기\n",
    "downsampling = 1e-3 # 문자 빈도 수 Downsample\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers=num_workers, \n",
    "                          size=num_features, \n",
    "                          min_count=min_word_count,\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 완료 되면 필요없는 메모리를 unload 시킨다.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "model_name = '300features_40minwords_10text'\n",
    "# model_name = '300features_50minwords_20text'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 결과 탐색 \n",
    "Exploring the Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도가 없는 단어 추출\n",
    "model.wv.doesnt_match('man woman child kitchen'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 유사한 단어를 추출\n",
    "model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"film\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.most_similar(\"happy\")\n",
    "model.wv.most_similar(\"happi\") # stemming 처리 시 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec으로 벡터화 한 단어를 t-SNE 를 통해 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고 https://stackoverflow.com/questions/43776572/visualise-word2vec-generated-from-gensim\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim \n",
    "import gensim.models as g\n",
    "\n",
    "# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "model_name = '300features_40minwords_10text'\n",
    "model = g.Doc2Vec.load(model_name)\n",
    "\n",
    "vocab = list(model.wv.vocab)\n",
    "X = model[vocab]\n",
    "\n",
    "print(len(X))\n",
    "print(X[0][:10])\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "# 100개의 단어에 대해서만 시각화\n",
    "X_tsne = tsne.fit_transform(X[:100,:])\n",
    "# X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_tsne, index=vocab[:100], columns=['x', 'y'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(40, 20)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos, fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    주어진 문장에서 단어 벡터의 평균을 구하는 함수\n",
    "    \"\"\"\n",
    "    # 속도를 위해 0으로 채운 배열로 초기화 한다.\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "\n",
    "    nwords = 0.\n",
    "    # Index2word는 모델의 사전에 있는 단어명을 담은 리스트이다.\n",
    "    # 속도를 위해 set 형태로 초기화 한다.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    # 루프를 돌며 모델 사전에 포함이 되는 단어라면 피처에 추가한다.\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # 결과를 단어수로 나누어 평균을 구한다.\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # 리뷰 단어 목록의 각각에 대한 평균 feature 벡터를 계산하고 \n",
    "    # 2D numpy 배열을 반환한다.\n",
    "    \n",
    "    # 카운터를 초기화 한다.\n",
    "    counter = 0.\n",
    "    # 속도를 위해 2D 넘파이 배열을 미리 할당한다.\n",
    "    reviewFeatureVecs = np.zeros(\n",
    "        (len(reviews),num_features),dtype=\"float32\")\n",
    "    \n",
    "    for review in reviews:\n",
    "       # 매 1000개 리뷰마다 상태를 출력\n",
    "       if counter%1000. == 0.:\n",
    "           print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       # 평균 피처 벡터를 만들기 위해 위에서 정의한 함수를 호출한다.\n",
    "       reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       # 카운터를 증가시킨다.\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티스레드로 4개의 워커를 사용해 처리한다.\n",
    "def getCleanReviews(reviews):\n",
    "    clean_reviews = []\n",
    "    clean_reviews = KaggleWord2VecUtility.apply_by_multiprocessing(\\\n",
    "        reviews[\"review\"], KaggleWord2VecUtility.review_to_wordlist,\\\n",
    "        workers=4)\n",
    "    return clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time trainDataVecs = getAvgFeatureVecs(\\\n",
    "    getCleanReviews(train), model, num_features ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time testDataVecs = getAvgFeatureVecs(\\\n",
    "        getCleanReviews(test), model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(\n",
    "    n_estimators = 100, n_jobs = -1, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time forest = forest.fit( trainDataVecs, train[\"sentiment\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "%time score = np.mean(cross_val_score(\\\n",
    "    forest, trainDataVecs, \\\n",
    "    train['sentiment'], cv=10, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = forest.predict( testDataVecs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv('data/Word2Vec_AverageVectors_{0:.5f}.csv'.format(score), \n",
    "              index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 300features_40minwords_10text 일 때 0.90709436799999987\n",
    "* 300features_50minwords_20text 일 때 0.86815798399999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
